import streamlit as st
import torch
import numpy as np
from PIL import Image
import plotly.express as px
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

import time
import av
import cv2
import re

import os
import sys
from pathlib import Path

from gliner import GLiNER
from sentence_transformers import SentenceTransformer
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, pipelines

# --- FIX LOGISTIQUE : CHARGEMENT DES DLL LIBVIPS (MODE PORTABLE) ---
# On utilise __file__ pour √™tre s√ªr de partir de l'emplacement du script
BASE_DIR = Path(__file__).parent.absolute()
VIPS_BIN_PATH = os.path.join(BASE_DIR, "vips-8.16", "bin")

if os.path.exists(VIPS_BIN_PATH):
    # 1. On l'ajoute au PATH syst√®me pour les sous-d√©pendances
    os.environ['PATH'] = VIPS_BIN_PATH + os.pathsep + os.environ['PATH']
    # 2. On force Python 3.12 √† accepter ce dossier pour les DLL
    os.add_dll_directory(VIPS_BIN_PATH)
    print(f"‚úÖ Libvips charg√© depuis : {VIPS_BIN_PATH}")
else:
    # Ce message appara√Ætra dans la console noire si le dossier n'est pas trouv√©
    print(f"‚ùå ERREUR : Dossier libvips introuvable √† {VIPS_BIN_PATH}")


# --- CONFIGURATION ---
st.set_page_config(page_title="IA Frugale", page_icon="‚úÇÔ∏è", layout="wide")

# --- INITIALISATION ---
if 'water' not in st.session_state: st.session_state.water = 0.0
if 'co2' not in st.session_state: st.session_state.co2 = 0.0
if 'last_res' not in st.session_state: st.session_state.last_res = None

# --- LA FONCTION MAGIQUE (Maintenant elle existe vraiment !) ---
def update_impact(ml, g):
    """Met √† jour les √©conomies d'eau et de CO2 dans la session"""
    st.session_state.water += ml
    st.session_state.co2 += g

# --- CSS HAUTE VISIBILIT√â (Texte blanc pur sur fond noir) ---
st.markdown("""
    <style>
    [data-testid="stSidebar"] { background-color: #0e1117; }
    .impact-card {
        background-color: #1E1E1E;
        padding: 20px;
        border-radius: 12px;
        border: 2px solid #00CCFF;
        text-align: center;
        margin-bottom: 20px;
    }
    .impact-label { color: #FFFFFF !important; font-weight: bold; font-size: 16px; margin-bottom: 0px; }
    .big-num { font-size: 32px; font-weight: bold; color: #00CCFF !important; margin-top: 0px; }
    
    .ds-container { display: flex; gap: 8px; margin: 20px 0; align-items: center; }
    .ds-box { padding: 10px 18px; border-radius: 6px; font-weight: bold; color: #333; background: #ddd; }
    .ds-active { color: white !important; box-shadow: 0 0 15px rgba(255,255,255,0.2); }
    .grade-a { background-color: #008000 !important; }
    .grade-b { background-color: #80FF00 !important; }
    </style>
    """, unsafe_allow_html=True)

def render_digiscore(grade):
    html = '<div class="ds-container"><span style="font-weight:bold;">Indice Frugalit√© :</span>'
    for g in ['A', 'B', 'C', 'D', 'E']:
        active = f"ds-active grade-{g.lower()}" if g == grade else ""
        html += f'<div class="ds-box {active}">{g}</div>'
    html += '</div>'
    st.markdown(html, unsafe_allow_html=True)

# --- CHARGEMENT DES MOD√àLES ---
@st.cache_resource
def load_models():
    # On fixe une r√©vision stable pour √©viter les mauvaises surprises
    MD_REVISION = "2025-01-09" 

    return {
        "gliner": GLiNER.from_pretrained("urchade/gliner_small-v2.1"),
        "embedder": SentenceTransformer('all-MiniLM-L6-v2'),
        "squeezer": pipeline("text-generation", model="HuggingFaceTB/SmolLM-135M-Instruct"),
        "vision": AutoModelForCausalLM.from_pretrained(
            "vikhyatk/moondream2", 
            trust_remote_code=True, 
            revision=MD_REVISION
        )
    }

models = load_models()
# 1. D√©finition de la fonction de chargement (avec cache pour la performance)
@st.cache_resource
def load_llm():
    """
    Charge un mod√®le tr√®s l√©ger (SmolLM2-135M) pour une ex√©cution locale rapide.
    Le cache permet de ne le charger qu'une seule fois en m√©moire.
    """
    # On utilise SmolLM2-135M-Instruct, parfait pour des d√©mos CPU rapides
    pipe = pipeline("text-generation", model="Qwen/Qwen2.5-0.5B-Instruct")
    return pipe

# --- MOTEUR D'ANONYMISATION ROBUSTE ---
def anonymize_text(text, model):
    # 1. On d√©finit TOUT ce qu'on veut que l'IA trouve
    labels = ["person", "location", "organization", "date", "job title", "amount", "address", "phone number"]
    entities = model.predict_entities(text, labels, threshold=0.3)
    
    # 2. On ajoute des Regex pour les structures fixes (S√©cu, IBAN, Mails)
    patterns = {
        "SOCIAL_SECURITY": r'[12][ ]?[0-9]{2}[ ]?[0-1][0-9][ ]?[0-9]{2,3}[ ]?[0-9]{3}[ ]?[0-9]{3}[ ]?[0-9]{2}',
        "IBAN": r'FR[0-9]{2}[ ]?([0-9]{4}[ ]?){5}[0-9]{3}',
        "EMAIL": r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\.[a-zA-Z0-9-.]+',
        "PHONE": r'(?:(?:\+|00)33|0)\s*[1-9](?:[\s.-]*\d{2}){4}'
    }
    
    # On compile toutes les zones √† cacher (NER + Regex)
    spans = []
    for ent in entities:
        spans.append((ent['start'], ent['end'], ent['label'].upper()))
    
    for label, pattern in patterns.items():
        for match in re.finditer(pattern, text):
            spans.append((match.start(), match.end(), label))

    # 3. Tri et fusion des zones pour √©viter les chevauchements (le bug du I[EMAIL]ement)
    spans.sort(key=lambda x: x[0])
    merged_spans = []
    if spans:
        curr_start, curr_end, curr_label = spans[0]
        for next_start, next_end, next_label in spans[1:]:
            if next_start < curr_end:
                curr_end = max(curr_end, next_end)
            else:
                merged_spans.append((curr_start, curr_end, curr_label))
                curr_start, curr_end, curr_label = next_start, next_end, next_label
        merged_spans.append((curr_start, curr_end, curr_label))

    # 4. Remplacement de la fin vers le d√©but pour garder les index valides
    result = text
    for start, end, label in reversed(merged_spans):
        result = result[:start] + f"[{label}]" + result[end:]
    return result


# --- SIDEBAR IMPACT ---
st.sidebar.title("üåç Impact Plan√©taire")
st.sidebar.markdown(f"""
<div class="impact-card">
    <p class="impact-label">üíß Eau √©conomis√©e</p>
    <p class="big-num">{round(st.session_state.water, 1)} ml</p>
    <p class="impact-label">‚òÅÔ∏è CO2 √©vit√©</p>
    <p class="big-num">{round(st.session_state.co2, 2)} g</p>
</div>
""", unsafe_allow_html=True)

tool = st.sidebar.radio("Modules :", ["üõ°Ô∏è GDPR Shield", "üß† Brain Map", "üé® Sketch2Code", "üìâ Token Squeezer", "üñêÔ∏è Hand Control"])

# Reset si changement d'onglet
if 'current_tool' not in st.session_state or st.session_state.current_tool != tool:
    st.session_state.current_tool = tool
    st.session_state.last_res = None

# =========================================================
# MODULE 1 : GDPR SHIELD
# =========================================================
if tool == "üõ°Ô∏è GDPR Shield":
    st.header("üõ°Ô∏è GDPR Shield : Anonymisation Forteresse")
    render_digiscore("A")
    
    sample_text = """M. Jean Martin, n√© le 3 septembre 1979 √† Bordeaux, r√©side au 42 rue de la Paix, 33100 Bordeaux. 
Il est mari√© √† Claire Lef√®vre. Son IBAN est FR76 3000 4000 1200 5678 9012 345.
Responsable logistique chez LogiTrans SA, son salaire est de 48 000 euros.
Num√©ro de s√©cu : 1 79 09 33 456 789 01. Contact : 06 88 21 45 09."""

    text_in = st.text_area("Texte sensible :", sample_text, height=250)
    
    if st.button("Lancer l'extraction locale"):
        with st.spinner("Analyse s√©mantique et filtrage local..."):
            res = anonymize_text(text_in, models['gliner'])
            st.session_state.last_res = res
            st.session_state.water += 500
            st.session_state.co2 += 2.5
        st.rerun()
    
    if st.session_state.last_res:
        st.markdown(f'<div class="result-area">{st.session_state.last_res}</div>', unsafe_allow_html=True)

# =========================================================
# MODULE 2 : Brain Map
# =========================================================
elif tool == "üß† Brain Map":
    st.header("üß† Brain Map : Intelligence Structurelle")
    render_digiscore("B")
    # L'INFO-CHOC (√Ä afficher tout de suite)
    col_a, col_b = st.columns(2)
    with col_a:
        st.metric("Mod√®le Local (Le v√¥tre)", "80 Mo", delta="L√©ger", delta_color="normal")
    with col_b:
        st.metric("Mod√®le Cloud (Standard)", "350 000 Mo", delta="4300x plus gros", delta_color="inverse")

    st.warning("‚ö†Ô∏è Pour trier vos 4 lignes, le Cloud mobilise un cerveau 4000 fois plus gros que n√©cessaire.")
    from sklearn.cluster import AgglomerativeClustering
    from sklearn.preprocessing import normalize

    # --- LE CURSEUR DE R√âGLAGE (La finesse) ---
    st.markdown("### üõ†Ô∏è R√©glage de la finesse")
    threshold = st.slider(
        "Sensibilit√© du regroupement (Threshold) :", 
        min_value=0.1, 
        max_value=0.9, 
        value=0.5, 
        step=0.05,
        help="Plus le seuil est BAS, plus l'IA est s√©v√®re et cr√©e de nombreux petits groupes pr√©cis. Plus il est HAUT, plus elle m√©lange les documents."
    )
    
    # Petit indicateur visuel pour aider l'utilisateur
    if threshold < 0.4:
        st.caption("üîç **Mode Chirurgical** : Id√©al pour s√©parer des documents tr√®s proches (ex: deux types de contrats).")
    elif threshold > 0.7:
        st.caption("üì¶ **Mode Global** : Id√©al pour voir les grandes masses (ex: Pro vs Perso).")
    else:
        st.caption("‚öñÔ∏è **Mode √âquilibr√©** : Le r√©glage standard.")


    default_docs = (
        "Facture √©lectricit√© Janvier\n"
        "Facture gaz F√©vrier\n"
        "R√©gularisation eau 2023\n"
        "Recette de cuisine : Tarte aux pommes\n"
        "Pr√©paration culinaire : Cr√™pes bretonnes\n"
        "Ingr√©dients g√¢teau au chocolat\n"
        "Devis r√©novation toiture\n"
        "Estimation isolation murs\n"
        "Contrat de travail CDD\n"
        "Avenant mutuelle entreprise\n"
        "Demande de cong√©s pay√©s"
    )

    raw_data = st.text_area("üì¶ Documents √† classer :", default_docs, height=200, key="t2")
    
    if st.button("Organiser intelligemment"):
        # On remplace les mots ambigus pour aider le petit mod√®le frugal
        docs = [d.strip() for d in raw_data.split('\n') if d.strip()]
        # Petit hack frugal pour lever l'ambigu√Øt√© sur "Recette"
        docs_for_ai = [d.replace("Recette", "Cuisine recette").replace("Facture", "Document comptable facture") for d in docs]
        
        if len(docs) > 2:
            with st.spinner("Analyse des vecteurs s√©mantiques..."):
                # 1. Vectorisation
                embeddings = models['embedder'].encode(docs_for_ai)
                # 2. Normalisation (Crucial pour la pr√©cision s√©mantique)
                embeddings = normalize(embeddings)
                
                # 3. Clustering Hi√©rarchique (plus pr√©cis que KMeans pour le texte)
                # On utilise une distance cosinus pour ignorer la taille des phrases
                cluster_model = AgglomerativeClustering(
                    n_clusters=None, 
                    distance_threshold=threshold, # Plus c'est bas, plus il cr√©e de petits groupes pr√©cis
                    metric='cosine', 
                    linkage='complete'
                )
                cluster_labels = cluster_model.fit_predict(embeddings)
                num_groups = len(set(cluster_labels))
                
                # 4. Projection 2D
                pca = PCA(n_components=2).fit_transform(embeddings)
                
                update_impact(300, 1.5)
                
                df = {
                    'x': pca[:, 0], 'y': pca[:, 1],
                    'Document': docs,
                    'Th√©matique': [f"Th√®me {l+1}" for l in cluster_labels]
                }
                
                fig = px.scatter(
                    df, x='x', y='y', text='Document', color='Th√©matique',
                    title=f"Organisation en {num_groups} th√©matiques distinctes",
                    template="plotly_dark",
                    color_discrete_sequence=px.colors.qualitative.Vivid
                )
                fig.update_traces(textposition='top center', marker=dict(size=14, line=dict(width=1, color='white')))
                fig.update_layout(dragmode='pan')
                fig.update_xaxes(showticklabels=False, title_text="Similitude th√©matique ‚Üí")
                fig.update_yaxes(showticklabels=False, title_text="Diff√©rence de contexte ‚Üë")
                
                st.session_state.last_res = {"fig": fig, "df": df}
            st.rerun()

    if st.session_state.last_res:
        st.plotly_chart(st.session_state.last_res["fig"], use_container_width=True)
        
        # Affichage des colonnes dynamique selon le nombre de th√®mes trouv√©s
        res_df = st.session_state.last_res["df"]
        themes = sorted(set(res_df['Th√©matique']))
        
        st.markdown("### üìã Classement automatique")
        # On cr√©e des colonnes (max 4 par ligne)
        for i in range(0, len(themes), 4):
            cols = st.columns(min(4, len(themes)-i))
            for j, theme in enumerate(themes[i:i+4]):
                with cols[j]:
                    st.info(f"**{theme}**")
                    items = [res_df['Document'][k] for k, t in enumerate(res_df['Th√©matique']) if t == theme]
                    for item in items:
                        st.write(f"‚Ä¢ {item}")

# =========================================================
# MODULE 3 : Sketch2Code
# =========================================================
elif tool == "üé® Sketch2Code":
    st.header("üé® Vision Frugale : Le dessin analys√©")
    render_digiscore("B")
    
    st.markdown("""
    **Le√ßon :** Un petit mod√®le local est parfait pour d√©crire une sc√®ne ou une personne. 
    Mais attention : sur des sch√©mas tr√®s complexes, le niveau de pr√©cision atteint toutefois ses limites.
    """)

    file = st.file_uploader("Image / Croquis", type=['png', 'jpg'], key="u3")
    if file:
        img = Image.open(file)
        st.image(img, width=400)
        
        # On sugg√®re des questions plus pr√©cises
        q_default = "D√©cris cette image pr√©cis√©ment en fran√ßais."
        prompt = st.text_input("Votre question :", q_default, key="p3")
        
        if st.button("Analyser l'image"):
            with st.spinner("L'IA locale analyse les pixels..."):
                update_impact(800, 5.0)
                
                 # 1. On pr√©pare le tokenizer (n√©cessaire pour le contr√¥le fin)
                from transformers import AutoTokenizer
                tokenizer = AutoTokenizer.from_pretrained("vikhyatk/moondream2", revision="2025-01-09")

                full_prompt = f"Identify and list all UI elements, colors, and icons in this image. Answer in French."

                # 3. On d√©compose l'appel pour ajouter les param√®tres de puissance
                enc_image = models['vision'].encode_image(img)
                
                # C'EST ICI QUE √áA CHANGE :
                answer = models['vision'].answer_question(
                    enc_image, 
                    full_prompt, 
                    tokenizer,
                    max_new_tokens=300,  # On lui donne le droit de parler longtemps (300 mots max)
                    iteration_count=3,   # On le force √† regarder l'image plusieurs fois
                )
                
                # Appel s√©curis√©
                try:
                    # On demande une r√©ponse un peu plus longue
                    result = models['vision'].query(img, full_prompt)
                    answer = result["answer"]
                    
                    # Si l'IA est t√™tue et r√©pond en anglais, on peut ajouter une note
                    st.session_state.last_res = answer
                except Exception as e:
                    st.session_state.last_res = f"Erreur d'analyse : {str(e)}"
            st.rerun()

    if st.session_state.last_res:
        st.subheader("Analyse de l'IA :")
        # On force l'affichage en fran√ßais si possible
        st.info(st.session_state.last_res)

# =========================================================
# MODULE 4 : TOKEN SQUEEZER (TEXT GEN)
# =========================================================
elif tool == "üìâ Token Squeezer":
    st.header("üìâ Token Squeezer (V3 - Mode Few-Shot)")
    st.caption("Strat√©gie : Donner des exemples au mod√®le pour forcer la bri√®vet√©.")

    user_prompt = st.text_area("Votre prompt verbeux :", 
                            "Je voudrais que tu agisses comme un expert en marketing et que tu m'√©crives un post pour LinkedIn qui parle de l'IA frugale, il faut que ce soit court, percutant, avec des emojis, et que √ßa explique pourquoi c'est √©colo.")

    if st.button("Compresser le Prompt"):
        with st.spinner("Chargement..."):
            generator = load_llm()
        
    # --- LA MAGIE EST ICI : FEW-SHOT PROMPTING ---
    # On donne des exemples "Avant -> Apr√®s" pour forcer le mod√®le √† imiter le style
    system_instruction = """Tu es un expert en compression de texte. 
    Ta t√¢che : Transformer des demandes longues en commandes imp√©ratives courtes.
    R√®gles : Supprime la politesse. Supprime 'Je veux que'. Utilise l'imp√©ratif. Pas de listes.

    Exemple 1 :
    Entr√©e : "Je voudrais que tu agisses comme un coach sportif et que tu me donnes un plan pour perdre du poids."
    Sortie : "Agis comme un coach sportif. Cr√©e un plan de perte de poids."

    Exemple 2 :
    Entr√©e : "Peux-tu √©crire un po√®me sur la pluie en style victorien s'il te plait ?"
    Sortie : "√âcris un po√®me victorien sur la pluie."

    Exemple 3 :
    Entr√©e : "Je veux une explication simple de la quantique pour un enfant de 5 ans."
    Sortie : "Explique la physique quantique √† un enfant de 5 ans."
    """
    
    # On construit le message final
    messages = [
        {"role": "system", "content": system_instruction},
        {"role": "user", "content": f"Entr√©e : \"{user_prompt}\"\nSortie :"}
    ]
    
    with st.spinner("Compression drastique..."):
        # Max token r√©duit pour couper la parole s'il devient bavard
        result = generator(messages, max_new_tokens=60, temperature=0.1) 
        
        output = result[0]['generated_text'][-1]['content']
        
        # Nettoyage final (parfois il laisse des guillemets)
        output = output.replace('"', '').strip()

        # Calculs
        len_original = len(user_prompt.split())
        len_optimized = len(output.split())
        reduction = ((len_original - len_optimized) / len_original) * 100
        
        col1, col2 = st.columns(2)
        with col1:
            st.warning(f"Original ({len_original} mots)")
            st.write(user_prompt)
        with col2:
            st.success(f"Optimis√© ({len_optimized} mots, -{int(reduction)}%)")
            st.code(output, language="text")

# =========================================================
# MODULE 5 : HAND CONTROL
# =========================================================
elif tool == "üñêÔ∏è Hand Control":
    st.header("üñêÔ∏è L'IA sans serveur")
    render_digiscore("A")
    import mediapipe as mp
    from streamlit_webrtc import webrtc_streamer, VideoProcessorBase
    
    class HandProcessor(VideoProcessorBase):
        def __init__(self): self.h = mp.solutions.hands.Hands(model_complexity=0)
        def recv(self, frame):
            img = cv2.flip(frame.to_ndarray(format="bgr24"), 1)
            results = self.h.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
            if results.multi_hand_landmarks:
                for lm in results.multi_hand_landmarks:
                    mp.solutions.drawing_utils.draw_landmarks(img, lm, mp.solutions.hands.HAND_CONNECTIONS)
            cv2.putText(img, "LOCAL - 0 WATER", (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            return av.VideoFrame.from_ndarray(img, format="bgr24")

    webrtc_streamer(key="hands", video_processor_factory=HandProcessor)
